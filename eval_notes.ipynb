{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Classification Model Performance\n",
    "\n",
    "## What is model evaluation?\n",
    "- Reality is the terrain. Models are a map. We can have different maps that have different levels of accuracy\n",
    "\n",
    "- Evaluating the reliability of our model predictions vs. reality tells us if we have a good map (good model)\n",
    "\n",
    "## So What?\n",
    "- We want models (maps) that are closer to reality than fiction\n",
    "\n",
    "## Now What?\n",
    "\n",
    "Key terms and concepts:\n",
    "\n",
    "Check engine light is a prediction.\n",
    "\n",
    "Check engine light goes off when there's a problem, then it's the \"positive case\".\n",
    "\n",
    "Whatever you're trying to test for is the positive case.\n",
    "\n",
    "If we're checking for churn with a \"churn predictor\", the positive case is \"churn\"\n",
    "\n",
    "The thing we're testing for associates w/ the affirmative, the positive.\n",
    "\n",
    "In the case of a check engine light:\n",
    "- A true positive: check engine light goes on, and there IS a problem is reality.\n",
    "- A false positive: check engine light goes on, but everything is OK. (false alarm)\n",
    "- A true negative: light stays off, engine is all good\n",
    "- A false negative: light stays off, but there's a problem with engine. (this is a Miss)\n",
    "\n",
    "If the value is 1 or True, that might be your positive case.\n",
    "\n",
    "Evaluating what we care most about (misses, false alarms) is a function of cost/benefit analysis and risk assessment\n",
    "\n",
    "- Say someone doesn't know vehicles well and it's a new vehicle, they're probably going to believe every check engine light\n",
    "\n",
    "\n",
    "## Boy Who Cried Wolf\n",
    "- false positive is crying wolf when there's no wolf. Villager risk assessment is to believe all prediction of wolf.\n",
    "- After 2 times of false positives, the villagers update their risk assessment. When there's a true positive, the villagers ignored\n",
    "\n",
    "## Most common metric for evaluating a model\n",
    "- accuracy: #(TP + TN) / #All prediction. % correct predictions. (TP - TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Let's say we make a cat predictor. Cat is the positive case. And all the data could be cat or dog.\n",
    "\n",
    "Let's say our train data is the following:\n",
    "Cat, Dog, Dog, Dog, Dog, Dog, Dog, Dog, Cat, Cat\n",
    "\n",
    "Positive case is: \"Cat\"\n",
    "Baseline prediction: our best guest or tool that ain't a ML model. \n",
    "Baseline prediction: with classification goals w/ a categorical target variable: the mode (the frequently occuring observation in the train)\n",
    "\n",
    "w/ Cat/Dog data, our baseline prediction is Dog\n",
    "\n",
    "positive = \"C\"\n",
    "actual = [\"CDDDDDDDCC\"]\n",
    "baseline = \"D\"\n",
    "predictions = [\"DDDDDDDDDD\"]\n",
    "TP = 0 # because we never predicted cat and cat is positive\n",
    "FP = 0 # because we never predicted cat at all\n",
    "TN = 7 # we predicted 10 dogs, reality was 7 dogs\n",
    "FN = 3 # we predicted 10 dogs, but reality was 3 cats\n",
    "\n",
    "accuracy = (TP + TN ) / #all =>  .70 accuracy\n",
    "\n",
    "We know we have a good model when we can beat baseline.\n",
    "Your model might not beat baseline. What does that mean? It means the features you trained your model on aren't good predictors.\n",
    "\n",
    "\n",
    "Same scenario as above:\n",
    "but we're building a Dog predictor\n",
    "positive_case: \"Dog\"\n",
    "actual = [\"CCCDDDDDDD\"]\n",
    "baselines = \"D\"\n",
    "predictions = [\"DDDDDDDDDD\"]\n",
    "\n",
    "TP = 7\n",
    "TN = 0\n",
    "FP = 3\n",
    "FN = 0\n",
    "accuracy = (TP + TN) / #observations = 0.7\n",
    "accuracy = (TP + TN) / (TP + TN + FP + TN) = 0.7"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
